\chapter{Grundlagen und verwandte Arbeiten}
\label{ch:background}


%
% Section: Der erste Abschnitt
%
\section{Related Work}
\label{sec:background:first_section}{TODO} 
In this section we will give an overview of related works. for this we conducted a intensive literature research with help of elicit and lumero. 



\subsection{Autonomous driving}
\label{subsec:background:first_section:first_subsection}


\subsection{Visual detection}
\label{subsec:background:first_section:second_subsection}


\subsection{Small devices, robotic etc}
\label{subsec:background:first_section:third_subsection}


%
% Section: Der Zweite Abschnitt
%
\section{Terminology and underlining Concepts}
\label{sec:background:second_section}
In this section we will provide basic information on the main terminology used for this work. We will provide a short description on each term and define what we understand in relation to our work.

Understanding Large Language Models (LLMs) and different architectures to leverage them in real-world applications is paramount to understand our approach and solution proposal for developing and smart autonomously operatiog LEGO train.

\subsection{Large Language Models (LLM)}
\label{subsec:background:second_section:first_subsection}

Large Language Models (LLMs) are deep neural network architectures (advanced deep learning algorithms) designed to understand and generate human language, making them capable of recognizing, summarizing, translating, predicting, and generating text. They are typically pre-trained on vast amounts of text data and fine-tuned for specific tasks. These models excel in completing sentences, generating creative content, and facilitating human-computer interaction through conversational agents.

In particular, LLMs like GPT-4 have demonstrated significant advances in understanding context, nuance, and complex queries (Cross et al., 2023; Ge et al., 2024). These models accelerate natural language processing applications like translation (Deepl, Google Translate), chatbots (Customer Service), and AI assistants (Siri, Alexa), broadening AI's impact in numerous sectors, making them versatile tools used across various industries, including healthcare, software development, and beyond. Their widespread application in various fields indicates their transformative potential in technology, education, healthcare, and more (Wang et al., 2024).

The operational efficiency of LLMs can be attributed to their ability to leverage a massive amount of data and sophisticated training algorithms. However, despite their capabilities, LLMs do not possess genuine understanding or reasoning akin to human cognition (Wu & Xiang, 2023). Instead, they operate by predicting the next word in a sequence based on patterns from their training data, leading to concerns regarding their potential to "hallucinate" false or misleading information (Ge et al., 2023). Furthermore, it is known that these models tend to be biased, leading to problems.

To enhance the power of these models meaning rasing their accuracy , one need to train the extensively, which is quite complex and requires a great deal of time and resource consumption.   

Furthermore on needs to have computers with lots of resources to use these large models effectively.

Edge-optimized implementations use techniques such as quantization, asynchronous processing, and selective
model invocation. Open-source models like Llama3-8b and fine-tuned smaller models run on consumer-grade
hardware, suggesting that a locally deployed LLM-based system on a Raspberry Pi 4 is feasible. 

\subsection{Agentic LLMs}
\label{subsec:background:second_section:second_subsection}

Agentic LLMs represent an extension of traditional LLMs, integrating autonomous decision-making capabilities with natural language processing. These models act as autonomous agents that can perform tasks, interact with users, and make decisions based on the incoming messages called prompts. Unlike traditional LLMs, agentic LLMs implement functionalities that enable action taking and the use of external tools based on user queries or environmental changes (Li et al., 2024; Li et al., 2024).

The "agentic" term suggests an integrated architecture in which LLMs not only generate text but also perform tasks and interact with their environment effectively. This capability sets the stage for the development of systems that can participate in collaborative environments, making them suitable for applications such as digital assistants, interactive systems in gaming, and automation in software development (Ge et al., 2024; Vallinder, 2022).


\subsection{Retrieval-Augmented Generation(RAG) Architectures}
\label{subsec:background:second_section:third_subsection}

Retrieval-Augmented Generation (RAG) is an architectural paradigm that combines LLMs with retrieval systems, enabling the model to access external information sources while generating responses. This approach significantly enhances the model's accuracy and relevance, especially in scenarios requiring up-to-date or domain-specific information. For example, LLMs can utilize a RAG system to pull relevant data from databases or documents during query processing, thus improving the quality of generated text for user inquiries (Wang et al., 2024; Rong et al., 2024).

A typical RAG application has two main components:

\begin{itemize}
    \item Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.
    \item Retrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.
\end{itemize}

\begin{description}
    \item[RAG Architecture] The fundamental RAG architecture involves retrieving relevant documents from a knowledge base and combining that retrieved information with the generative capabilities of LLMs to produce contextually enriched responses.

    \item[Conditional RAG (CRAG)] This variant incorporates conditional features that refine the retrieval process by focusing on user-specific conditions or prompts, tailoring the retrieved context more closely to user intent. This increases the effectiveness of the interaction by ensuring that the provided responses are more personalized and context-aware.

    \item[Self-RAG] Self-RAG involves models that can autonomously identify when external information is needed and retrieve it as part of their operational flow. This type of architecture further enhances the reasoning capability of the model, allowing it to assess its current task and decide when additional context or data is warranted for optimal performance (Rong et al., 2024; Yang et al., 2024).

\end{description}

\subsection{Relation Between LLMs, Agentic LLMs, and RAG Architectures}
\label{subsec:background:second_section:fourth_subsection}

The integration of LLMs into RAG architectures forms a robust foundation for developing agentic behaviors. Agentic LLMs benefit from RAG mechanisms by accessing dynamic external databases, ensuring that their responses are accurate and relevant while maintaining a conversational context. The relation is multifaceted:

\begin{description}
    \item[Enhanced Context] Agentic LLMs can leverage RAG systems to manage complex queries by retrieving up-to-date information that informs their responses, thereby enhancing overall user engagement and satisfaction.
    
    \item[Improved Decision-Making] Utilizing CRAG or Self-RAG architectures augments the reasoning capabilities of agentic LLMs, enabling them to execute tasks based on contextual awareness rather than relying solely on static datasets. This shift toward dynamic decision-making enhances their capability to serve in diverse applications, from customer support to medical diagnosis (Yang et al., 2024; Yu et al., 2024).
    
    \item[Operational Efficiency] By integrating agentic structures with RAG frameworks, developers can create systems that not only generate human-like text but also function effectively in real-world scenarios, interact autonomously, and adapt to user needs.
\end{description}
 

In summary, large language models (LLMs) and agentic LLMs represent pivotal advancements in the field of artificial intelligence, particularly in natural language processing. Their integration with retrieval-augmented generation (RAG) architectures, such as CRAG and Self-RAG, enhances their capabilities, promoting a new wave of intelligent agents capable of reasoning, adapting, and interacting effectively with users. The ongoing research and development in this domain holds considerable promise for creating more sophisticated AI applications capable of transforming user experiences across various sectors.

 