%*******************************************************
% Abstract in English
%*******************************************************
\pdfbookmark[0]{Abstract}{Abstract}


\begin{otherlanguage}{american}
	\chapter*{Abstract}
	
This thesis explores how agentic large language models (LLMs) can be deployed on resource-limited devices like a Raspberry Pi to autonomously control a LEGO train. The idea came from combining cutting-edge AI with hands-on robotics in an educational setting, especially for workshops like KinderCampus. Itâ€™s an interesting challenge because most LLMs are designed to run in the cloud, but here the goal was to build a fully offline system that is private, low-cost, and transparent.

The system I built includes local speech recognition, object detection, a graphical interface, and an LLM agent using the LangGraph framework. All components run on a single Raspberry Pi and communicate using ZeroMQ. The agent is able to reason about commands, decide whether to call a tool, and keep track of runtime state. I designed the architecture to be modular and easy to understand, especially for students. The implementation followed best practices such as separating logic into modules, using background threads carefully, and handling errors gracefully.

In the end, the project worked: the train can be controlled by voice, reacts to obstacles, and the whole setup runs without needing an Internet connection. It also serves as a platform for teaching the design of real-world AI systems. Students can modify parts, add tools, and learn how LLMs can be used for safe, interactive control on limited hardware. The system is not just a demo, it is something others can actually build on and learn from.

\end{otherlanguage}
